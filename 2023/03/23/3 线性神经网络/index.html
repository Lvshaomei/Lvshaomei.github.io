<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="keywords" content="LSM, Mia's blog"><meta name="description" content="生活总是需要不断的跌倒，再不断的爬起来，这就是生活。"><title>3 线性神经网络 | Mia's blog</title><meta name="google-site-verification" content="HPcpCEPE_TQ7LlH-LAToYJleDQ7ydr0Mgiw95TQ5GNQ"><meta name="msvalidate.01" content="9C903B945C2FE1FB1A111163F0C32703"><meta name="theme-color" content="#54bcff"><link rel="manifest" href="../../../../manifest.json?v=1.5.4"><link rel="icon" href="../../../../images/icons/Lv-16.png?v=1.5.4" type="image/png" sizes="16x16"><link rel="icon" href="../../../../images/icons/Lv-32.png?v=1.5.4" type="image/png" sizes="32x32"><link rel="alternate" href="../../../../atom.xml" type="application/atom+xml"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" type="text/css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.css" type="text/css"><link rel="stylesheet" href="../../../../css/index.css?v=1.5.4"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="dns-prefetch" href="https://hm.baidu.com"><link rel="dns-prefetch" href="https://tajs.qq.com"><script src="https://www.googletagmanager.com/gtag/js?id=UA-156048106-1" async=""></script><script>if (window.location.hostname !== 'localhost') {
  window.dataLayer = window.dataLayer || [];
  function gtag(){ dataLayer.push(arguments); }
  gtag('js', new Date());
  gtag('config', 'UA-156048106-1');
}</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement('script');
  hm.src = 'https://hm.baidu.com/hm.js?3df607e8ce8da7ad737b9205f02966ee';
  hm.async = true;

  if (false) { hm.setAttribute('data-pjax', ''); }

  var s = document.getElementsByTagName('script')[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>(function() {
  var hm = document.createElement('script');
  hm.src = 'https://tajs.qq.com/stats?sId=undefined';
  hm.async = true;

  if (false) { hm.setAttribute('data-pjax', ''); }

  var s = document.getElementsByTagName('script')[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  fontawesome: {"prefix":"fa"},
  sidebar: {"offsetTop":"30px","renderTocDepth":4},
  header: {"scrollDownIcon":true},
  back2top: {"enable":true},
  reward: false,
  fancybox: false,
  zoom_image: {"enable":true,"mask_color":"rgba(0,0,0,0.6)"},
  gallery_waterfall: undefined,
  lazyload: undefined,
  pjax: undefined,
  external_link: {"icon":{"enable":true,"name":"external-link"}},
  shortcuts: {"switch_post":false},
  prompt: {"copy_success":"复制成功","copy_error":"复制失败","creative_commons":"知识共享","copy_button":"点击复制"}
};

window.CONFIG = CONFIG;</script></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav"><div class="header-nav-inner"><div class="header-nav-btn fa fa-bars"></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/"><i class="fa fa-home"></i>首页</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="javascript:;" target="_blank" rel="noopener"><i class="fa fa-paint-brush"></i>文章</a><div class="header-nav-submenu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/archives/"><i class="fa fa-folder-open"></i>归档</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/categories/"><i class="fa fa-th"></i>分类</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/tags/"><i class="fa fa-tags"></i>标签</a></div></div></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/about/"><i class="fa fa-user"></i>关于</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/friends/"><i class="fa fa-hand-spock-o"></i>友链</a></div></div><div class="header-nav-search"><i class="fa fa-search"></i><span>搜索</span></div></div></nav><div class="header-info"><div class="header-info-inner"><div class="header-info-title">Mia's blog</div><div class="header-info-subtitle">留意身边的美好</div></div><div class="header-info-scrolldown"><i class="fa fa-angle-down header-info-scrolldown__icon"></i></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content"><div class="post"><header class="post-header"><h1 class="post-header-title">3 线性神经网络</h1><div class="post-header-meta"><span class="post-header-meta-create"><i class="fa fa-calendar-o"></i><span>发表于 </span><span>2023-03-23</span></span><span class="post-header-meta-update"><i class="fa fa-calendar-check-o"></i><span>更新于 </span><span>2023-03-23</span></span><span class="post-header-meta-word-count"><i class="fa fa-file-word-o"></i><span>字数统计 </span><span>1.7k</span></span><span class="post-header-meta-reading-time"><i class="fa fa-clock-o"></i><span>阅读时长 </span><span>10m</span></span><span class="post-header-meta-reading-count"><i class="fa fa-eye"></i><span>阅读次数 </span><span id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body"><h1 id="3-1-线性回归"><span class="heading-link">3.1 线性回归</span></h1><h2 id="小结"><span class="heading-link">小结</span></h2><ol>
<li><strong>由果到因</strong>，根据已经发生的观测结果去猜想真实参数，这个过程叫做<strong>估计</strong>；估计正确的可能性叫做<strong>似然性</strong>。求可能性最大的推测，这个过程就是<strong>极大似然估计</strong>。由因到果，根据真实参数（或已经发生的观测结果）去推测未来的观测结果，这个过程叫做<strong>预测</strong>；预测正确的可能性叫做<strong>概率</strong>。</li>
<li><strong>损失函数</strong>：能够量化目标的实际值与预测值之间的差距。</li>
<li><strong>解析解</strong>：可以用一个公式简单地表达出来。</li>
<li><strong>随机梯度下降</strong>：通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做小批量随机梯度下降（minibatch stochastic gradient descent）</li>
<li>算法的步骤如下：（1）初始化模型参数的值，如随机初始化； （2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。</li>
<li><strong>超参数</strong>：可以调整但不在训练过程中更新的参数。<strong>调参</strong>：选择超参数的过程。</li>
<li><strong>泛化</strong>（generalization）：找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失。</li>
</ol>
<h2 id="练习"><span class="heading-link">练习</span></h2><ol>
<li>假设我们有一些数据$x_1, \ldots, x_n \in \mathbb{R}$。我们的目标是找到一个常数$b$，使得最小化$\sum_i (x_i - b)^2$。 <ol>
<li>找到最优值$b$的解析解。</li>
<li>这个问题及其解与正态分布有什么关系?</li>
</ol>
</li>
</ol>
<p><img src="https://cdn.nlark.com/yuque/0/2023/jpeg/1866697/1679575137426-6355cc99-8deb-4003-9f8b-56ae9e17fcdb.jpeg?x-oss-process=image/auto-orient,1#averageHue=%23c6bfaf&amp;clientId=u257d3d6a-dacf-4&amp;from=paste&amp;height=467&amp;id=udec6df17&amp;name=f220011d9f56d741001e058085bf3d7.jpg&amp;originHeight=700&amp;originWidth=1819&amp;originalType=binary&amp;ratio=1.5&amp;rotation=0&amp;showTitle=false&amp;size=193443&amp;status=done&amp;style=none&amp;taskId=u9eb22437-f7ec-4517-8e17-14d38d51d00&amp;title=&amp;width=1212.6666666666667" alt="f220011d9f56d741001e058085bf3d7.jpg"><br><img src="https://cdn.nlark.com/yuque/0/2023/jpeg/1866697/1679575164638-f9afce37-486c-4994-9ff9-1b9c2a508fb1.jpeg?x-oss-process=image/auto-orient,1#averageHue=%23bcb5a6&amp;clientId=u257d3d6a-dacf-4&amp;from=paste&amp;height=629&amp;id=u2fa4c110&amp;name=4b4a7a7be951be654ca83a2fc8091c2.jpg&amp;originHeight=944&amp;originWidth=1470&amp;originalType=binary&amp;ratio=1.5&amp;rotation=0&amp;showTitle=false&amp;size=228144&amp;status=done&amp;style=none&amp;taskId=u50d9aff1-4102-41ee-b704-25c53806b5b&amp;title=&amp;width=980" alt="4b4a7a7be951be654ca83a2fc8091c2.jpg"></p>
<ol start="2">
<li>推导出使用平方误差的线性回归优化问题的解析解。为了简化问题，可以忽略偏置$b$（我们可以通过向$\mathbf X$添加所有值为1的一列来做到这一点）。 <ol>
<li>用矩阵和向量表示法写出优化问题（将所有数据视为单个矩阵，将所有目标值视为单个向量）。</li>
<li>计算损失对$w$的梯度。</li>
<li>通过将梯度设为0、求解矩阵方程来找到解析解。</li>
<li>什么时候可能比使用随机梯度下降更好？这种方法何时会失效？</li>
</ol>
</li>
</ol>
<p><img src="https://cdn.nlark.com/yuque/0/2023/jpeg/1866697/1679575198670-0cf0dde2-4e9f-4f4f-81bb-d9252d0b2ffc.jpeg?x-oss-process=image/auto-orient,1#averageHue=%23c2bcad&amp;clientId=u257d3d6a-dacf-4&amp;from=paste&amp;height=485&amp;id=u8edc42a5&amp;name=20a3cd63e0e9b71742d6ea41a6c4ad2.jpg&amp;originHeight=727&amp;originWidth=1402&amp;originalType=binary&amp;ratio=1.5&amp;rotation=0&amp;showTitle=false&amp;size=178129&amp;status=done&amp;style=none&amp;taskId=ub288724b-e8bf-461b-8c7e-a2da8d4e4cf&amp;title=&amp;width=934.6666666666666" alt="20a3cd63e0e9b71742d6ea41a6c4ad2.jpg"><br>d: 当模型简单的时候，通过求W的解析解是比随机梯度下降更好，但是当$X^{T}X$不可逆时，无法求出解析解。</p>
<ol start="3">
<li>假定控制附加噪声$\epsilon$的噪声模型是指数分布。也就是说，$p(\epsilon) = \frac{1}{2} \exp(-|\epsilon|)$ <ol>
<li>写出模型$-\log P(\mathbf y \mid \mathbf X)$下数据的负对数似然。</li>
<li>请试着写出解析解。</li>
<li>提出一种随机梯度下降算法来解决这个问题。哪里可能出错？（提示：当我们不断更新参数时，在驻点附近会发生什么情况）请尝试解决这个问题。</li>
</ol>
</li>
</ol>
<p><img src="https://cdn.nlark.com/yuque/0/2023/jpeg/1866697/1679575242913-4a360559-4f73-4c71-a36b-1da0082833c8.jpeg?x-oss-process=image/auto-orient,1#averageHue=%23c8c1b1&amp;clientId=u257d3d6a-dacf-4&amp;from=paste&amp;height=660&amp;id=u0b242022&amp;name=714017c91023bf8de42d41d88cdb4df.jpg&amp;originHeight=990&amp;originWidth=1432&amp;originalType=binary&amp;ratio=1.5&amp;rotation=0&amp;showTitle=false&amp;size=221078&amp;status=done&amp;style=none&amp;taskId=u4cd9a677-e658-4de3-a627-c9b89af3ca3&amp;title=&amp;width=954.6666666666666" alt="714017c91023bf8de42d41d88cdb4df.jpg"><br>线性绝对值函数在极点处是没有导数的。<br>c： 所求得的损失函数其实是1范数的形式，在驻点处不可导。<br>梯度下降法可能碰到问题： 例如在驻点附近，参数剧烈波动难以收敛，所以可以当损失函数小于一定阈值后，就用2范数代替1范数，即避免了二范数在距离驻点较远时梯度太大训练不稳定，也避免了1范数在驻点附近参数剧烈波动难以收敛。</p>
<h1 id="3-2-线性回归的从零开始实现"><span class="heading-link">3.2 线性回归的从零开始实现</span></h1><h2 id="练习-1"><span class="heading-link">练习</span></h2><ol>
<li>如果我们将权重初始化为零，会发生什么。算法仍然有效吗？</li>
</ol>
<p>在单层网络中，将权重初始化为零时可以的，但是网络层数加深后，在全连接的情况下，在反向传播的时候，由于权重的对称性会导致出现隐藏神经元的对称性，使得多个隐藏神经元的作用就如同1个神经元，算法还是有效的，但是效果不大好。（<span class="external-link"><a href="https://zhuanlan.zhihu.com/p/75879624" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/75879624</a><i class="fa fa-external-link"></i></span>）</p>
<ol start="2">
<li>假设试图为电压和电流的关系建立一个模型。自动微分可以用来学习模型的参数吗?</li>
</ol>
<p>可以，建立模型U=IW+b，建立模型U=IW+b</p>
<ol start="3">
<li><p>能基于<span class="external-link"><a href="https://en.wikipedia.org/wiki/Planck%27s_law" target="_blank" rel="noopener">普朗克定律</a><i class="fa fa-external-link"></i></span>使用光谱能量密度来确定物体的温度吗？</p>
</li>
<li><p>计算二阶导数时可能会遇到什么问题？这些问题可以如何解决？</p>
</li>
</ol>
<p>一阶导数的正向计算图无法直接获得，可以通过保存一阶导数的计算图使得可以求二阶导数</p>
<ol start="5">
<li>为什么在<code>squared_loss</code>函数中需要使用<code>reshape</code>函数？</li>
</ol>
<p>以防y^和y，一个是行向量、一个是列向量，使用reshape，可以确保shape一样。</p>
<ol start="6">
<li>尝试使用不同的学习率，观察损失函数值下降的快慢。</li>
</ol>
<p>①学习率过大前期下降很快，但是后面不容易收敛；<br>②学习率过小损失函数下降会很慢。</p>
<ol start="7">
<li>如果样本个数不能被批量大小整除，<code>data_iter</code>函数的行为会有什么变化？</li>
</ol>
<p>出错</p>
<h1 id="3-3-线性回归的简洁实现"><span class="heading-link">3.3 线性回归的简洁实现</span></h1><h2 id="小结-1"><span class="heading-link">小结</span></h2><ol>
<li>Sequential类将多个层串联在一起。当给定输入数据时，Sequential实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。</li>
<li>通过net[0]选择网络中的第一个图层， 然后使用weight.data和bias.data方法访问参数。还可以使用替换方法normal_和fill_来重写参数值。</li>
<li><strong>计算均方误差使用的是MSELoss类，也称为平方𝐿2范数</strong>]。 默认情况下，它返回所有样本损失的平均值。<code>loss = nn.MSELoss()</code></li>
<li><strong>实例化一个SGD实例</strong>)时，我们要指定优化的参数 （可通过net.parameters()从我们的模型中获得）以及优化算法所需的超参数字典。<code>trainer = torch.optim.SGD(net.parameters(), lr=0.03)</code></li>
<li>对于每一个小批量，我们会进行以下步骤:</li>
</ol>
<ul>
<li>通过调用net(X)生成预测并计算损失l（前向传播）。</li>
<li>通过进行反向传播来计算梯度。</li>
<li>通过调用优化器来更新模型参数。</li>
</ul>
<h2 id="练习-2"><span class="heading-link">练习</span></h2><ol>
<li>如果将小批量的总损失替换为小批量损失的平均值，需要如何更改学习率？</li>
</ol>
<p>将学习率除以batchsize。</p>
<ol start="2">
<li>查看深度学习框架文档，它们提供了哪些损失函数和初始化方法？用Huber损失代替原损失，即$l(y,y’) = \begin{cases}|y-y’| -\frac{\sigma}{2} &amp; \text{ if } |y-y’| &gt; \sigma \ \frac{1}{2 \sigma} (y-y’)^2 &amp; \text{ 其它情况}\end{cases}$ </li>
</ol>
<ol start="3">
<li>如何访问线性回归的梯度？<figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.grad</span><br><span class="line">net[<span class="number">0</span>].bias.grad</span><br></pre></td></tr></tbody></table></div></figure>
</li>
</ol>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><footer class="post-footer"><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-author-name">本文作者: </span><span class="post-copyright-author-value"><a href="https://lvshaomei.github.io">LSM</a></span></div><div class="post-copyright-link"><span class="post-copyright-link-name">本文链接: </span><span class="post-copyright-link-value"><a href="https://lvshaomei.github.io/2023/03/23/3%20%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">https://lvshaomei.github.io/2023/03/23/3%20%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-notice-name">版权声明: </span><span class="post-copyright-notice-value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-cn" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><nav class="paginator"><div class="paginator-post"><div class="paginator-post-prev"><a class="paginator-post-prev__a" href="../../../10/24/%E5%B8%A6%E4%BD%A0%E7%94%A8%E6%B2%B9%E7%8C%B4%E6%8F%92%E4%BB%B6%E5%85%8D%E8%B4%B9%E8%A7%82%E7%9C%8B%E5%90%84%E5%A4%A7%E5%B9%B3%E5%8F%B0%EF%BC%88B%E7%AB%99%E3%80%81%E7%88%B1%E5%A5%87%E8%89%BA%E7%AD%89%EF%BC%89VIP%E8%A7%86%E9%A2%91/"><i class="fa fa-chevron-left"></i><span>带你用油猴插件免费观看各大平台（B站、爱奇艺等）VIP视频</span></a></div><div class="paginator-post-next"><a class="paginator-post-next__a" href="../../07/%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E6%9C%9F%E4%B8%AD%E5%A4%A7%E4%BD%9C%E4%B8%9A/"><span>第十章 期中大作业</span><i class="fa fa-chevron-right"></i></a></div></div></nav></footer></div></div><div class="comments" id="comments"><div id="gitalk-container"></div></div></div><aside class="sidebar" id="sidebar"><div class="sidebar-inner"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#3-1-线性回归"><span class="toc-number">1.</span> <span class="toc-text">3.1 线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#小结"><span class="toc-number">1.1.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#练习"><span class="toc-number">1.2.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-2-线性回归的从零开始实现"><span class="toc-number">2.</span> <span class="toc-text">3.2 线性回归的从零开始实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#练习-1"><span class="toc-number">2.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-3-线性回归的简洁实现"><span class="toc-number">3.</span> <span class="toc-text">3.3 线性回归的简洁实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#小结-1"><span class="toc-number">3.1.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#练习-2"><span class="toc-number">3.2.</span> <span class="toc-text">练习</span></a></li></ol></li></ol></div></section><!-- ov = overview --><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/banner/labia.jpeg" alt="avatar"></div><p class="sidebar-ov-author__p">The Harder You Work, The Luckier You Will Be.</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social__item" href="https://github.com/Lvshaomei/Lvshaomei.github.io" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><i class="sidebar-ov-social__item--icon fa fa-github"></i></a><a class="sidebar-ov-social__item" href="https://segmentfault.com/u/lvlv_5e182fd7c6adc" target="_blank" rel="noopener" data-popover="思否" data-popover-pos="up"><span class="sidebar-ov-social__item--logo">sf</span></a><a class="sidebar-ov-social__item" href="https://www.zhihu.com/people/fa-liang-jian-shao/activities" target="_blank" rel="noopener" data-popover="知乎" data-popover-pos="up"><span class="sidebar-ov-social__item--logo">�</span></a><a class="sidebar-ov-social__item" href="https://juejin.im/user/5da86e3d5188257948602ace" target="_blank" rel="noopener" data-popover="social.juejin" data-popover-pos="up"><span class="sidebar-ov-social__item--logo">�</span></a></div><div class="sidebar-ov-feed"><span class="sidebar-ov-feed-rss"><a class="sidebar-ov-feed-rss__a" href="../../../../atom.xml" target="_blank" rel="noopener"><i class="sidebar-ov-feed-rss__a--icon fa fa-rss"></i><span>RSS </span><span>订阅</span></a></span></div><div class="sidebar-ov-state"></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-cn" target="_blank" rel="noopener" data-popover="知识共享" data-popover-pos="up"><img src="../../../../images/cc-by-nc-sa.svg" alt="知识共享"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span>你已阅读了 </span><span class="sidebar-reading-info-num">0</span></div><div class="sidebar-reading-line"></div></div></div></aside><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>&copy; 2023</span><span class="fa fa-heart footer-icon"></span><span>LSM.</span></div><div><span>由 <a href="http://hexo.io/" title="hexo" target="_blank" rel="noopener">hexo</a> 强力驱动</span><span> v4.0.0.</span><span class="separator">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="stun" target="_blank" rel="noopener">stun</a></span><span> v1.5.4.</span></div><div class="busuanzi"><span class="busuanzi-site_uv"><i class="fa fa-user"></i><span>访问人数 </span><span id="busuanzi_value_site_uv"></span></span><span class="separator">|</span><span class="busuanzi-site_pv"><i class="fa fa-eye"></i><span>浏览总量 </span><span id="busuanzi_value_site_pv"></span></span><script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@latest/bsz.pure.mini.js" async></script></div></div></footer><div class="loading-bar" id="loading-bar"><div class="progress"></div></div><div class="back2top" id="back2top"><i class="back2top-icon fa fa-rocket"></i></div></div><div class="search-mask"></div><div class="search-popup"><span class="search-close"></span><div class="search-input"><input placeholder="搜索文章（支持多关键词，请用空格分隔）"></div><div class="search-results"></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/ribbon.js@latest/dist/ribbon.min.js" size="120" alpha="0.6" zIndex="-1"></script><script src="https://cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@latest/canvas-nest.min.js" color="243,81,151" opacity="0.5" count="100" zIndex="-2"></script><script>window.addEventListener('DOMContentLoaded', function () {
  $('.header-nav-search').on('click', function (e) {
    e.stopPropagation();

    $('body').css('overflow', 'hidden');
    $('.search-popup')
      .addClass('show')
      .velocity('stop')
      .velocity('transition.expandIn', {
        duration: 300,
        complete: function () {
          $('.search-popup input').focus();
        }
      });
    $('.search-mask')
      .velocity('stop')
      .velocity('transition.fadeIn', {
        duration: 300
      });

    initSearch();
  });

  $('.search-mask, .search-close').on('click', function () {
    closeSearch();
  });

  $(document).on('keydown', function (e) {
    // Escape <=> 27
    if (e.keyCode === Stun.utils.codeToKeyCode('Escape')) {
      closeSearch();
    }
  });

  var isXML = true;
  var search_path = 'search.xml';

  if (!search_path) {
    search_path = 'search.xml';
  } else if (/json$/i.test(search_path)) {
    isXML = false;
  }

  var path = '/' + search_path;

  function initSearch() {
    $.ajax({
      url: path,
      dataType: isXML ? 'xml' : 'json',
      async: true,
      success: function (res) {
        var datas = isXML ? $('entry', res).map(function () {
          // 将 XML 转为 JSON
          return {
            title: $('title', this).text(),
            content: $('content', this).text(),
            url: $('url', this).text()
          };
        }).get() : res;

        var $input = $('.search-input input');
        var $result = $('.search-results');

        // 搜索对象（标题、内容）的权重，影响显示顺序
        var WEIGHT = { title: 100, content: 1 };

        var searchPost = function () {
          var searchText = $input.val().toLowerCase().trim();
          // 根据空白字符分隔关键字
          var keywords = searchText.split(/[\s]+/);
          // 搜索结果
          var matchPosts = [];

          // 有多个关键字时，将原文字整个保存下来
          if (keywords.length > 1) {
            keywords.push(searchText);
          }

          // 防止未输入字符时搜索
          if (searchText.length > 0) {
            datas.forEach(function (data) {
              var isMatch  = false;

              // 没有标题的文章使用预设的 i18n 变量代替
              var title = (data.title && data.title.trim()) || '( 文章无标题 )';
              var titleLower = title && title.toLowerCase();
              // 删除 HTML 标签 和 所有空白字符
              var content = data.content && data.content.replace(/<[^>]+>/g, '');
              var contentLower = content && content.toLowerCase();
              // 删除重复的 /
              var postURL = data.url && decodeURI(data.url).replace(/\/{2,}/g, '/');

              // 标题中匹配到的关键词
              var titleHitSlice = [];
              // 内容中匹配到的关键词
              var contentHitSlice = [];

              keywords.forEach(function (keyword) {
                /**
                  * 获取匹配的关键词的索引
                  * @param {String} keyword 要匹配的关键字
                  * @param {String} text 原文字
                  * @param {Boolean} caseSensitive 是否区分大小写
                  * @param {Number} weight 匹配对象的权重。权重大的优先显示
                  * @return {Array}
                  */
                function getIndexByword (word, text, caseSensitive, weight) {
                  if (!word || !text) return [];

                  var startIndex = 0; // 每次匹配的开始索引
                  var index = -1;     // 匹配到的索引值
                  var result = [];    // 匹配结果

                  if (!caseSensitive) {
                    word = word.toLowerCase();
                    text = text.toLowerCase();
                  }

                  while((index = text.indexOf(word, startIndex)) !== -1) {
                    var hasMatch = false;

                    // 索引位置相同的关键词，保留长度较长的
                    titleHitSlice.forEach(function (hit) {
                      if (hit.index === index && hit.word.length < word.length) {
                        hit.word = word;
                        hasMatch = true;
                      }
                    });
                    startIndex = index + word.length;
                    !hasMatch && result.push({ index: index, word: word, weight: weight });
                  }

                  return result;
                }

                titleHitSlice = titleHitSlice.concat(getIndexByword(keyword, titleLower, false, WEIGHT.title));
                contentHitSlice = contentHitSlice.concat(getIndexByword(keyword, contentLower, false, WEIGHT.content));
              });

              var hitTitle = titleHitSlice.length;
              var hitContent = contentHitSlice.length;

              if (hitTitle > 0 || hitContent > 0) {
                isMatch = true;
              }

              if (isMatch) {
                ;[titleHitSlice, contentHitSlice].forEach(function (hit) {
                  // 按照匹配文字的索引的递增顺序排序
                  hit.sort(function (left, right) {
                    return left.index - right.index;
                  });
                });

                /**
                  * 给文本中匹配到的关键词添加标记，从而进行高亮显示
                  * @param {String} text 原文本
                  * @param {Array} hitSlice 匹配项的索引信息
                  * @param {Number} start 开始索引
                  * @param {Number} end 结束索引
                  * @return {String}
                  */
                function highlightKeyword (text, hitSlice, start, end) {
                  if (!text || !hitSlice || !hitSlice.length) return;

                  var result = '';
                  var startIndex = start;
                  var endIndex = end;

                  hitSlice.forEach(function (hit) {
                    if (hit.index < startIndex) return;

                    var hitWordEnd = hit.index + hit.word.length;

                    result += text.slice(startIndex, hit.index);
                    result += '<b>' + text.slice(hit.index, hitWordEnd) + '</b>';
                    startIndex = hitWordEnd;
                  });
                  result += text.slice(startIndex, endIndex);

                  return result;
                }

                var postData = {};
                // 文章总的搜索权重
                var postWeight = titleHitSlice.length * WEIGHT.title + contentHitSlice.length * WEIGHT.content;
                // 标记匹配关键词后的标题
                var postTitle = highlightKeyword(title, titleHitSlice, 0, title.length) || title;
                // 标记匹配关键词后的内容
                var postContent;
                // 显示内容的长度
                var SHOW_WORD_LENGTH = 200;
                // 命中关键词前的字符显示长度
                var SHOW_WORD_FRONT_LENGTH = 20;
                var SHOW_WORD_END_LENGTH = SHOW_WORD_LENGTH - SHOW_WORD_FRONT_LENGTH;

                // 截取匹配的第一个字符，前后共 200 个字符来显示
                if (contentHitSlice.length > 0) {
                  var firstIndex = contentHitSlice[0].index;
                  var start = firstIndex > SHOW_WORD_FRONT_LENGTH ? firstIndex - SHOW_WORD_FRONT_LENGTH : 0;
                  var end = firstIndex + SHOW_WORD_END_LENGTH;

                  postContent = highlightKeyword(content, contentHitSlice, start, end);
                } else { // 未匹配到内容，直接截取前 200 个字符来显示
                  postContent = content.slice(0, SHOW_WORD_LENGTH);
                }

                postData.title = postTitle;
                postData.content = postContent;
                postData.url = postURL;
                postData.weight = postWeight;
                matchPosts.push(postData);
              }
            });
          }

          var resultInnerHtml = '';

          if (matchPosts.length) {
            // 按权重递增的顺序排序，使权重大的优先显示
            matchPosts.sort(function (left, right) {
              return right.weight - left.weight;
            });

            resultInnerHtml += '<ul>';
            matchPosts.forEach(function (post) {
              resultInnerHtml += '<li><a class="search-results-title" href="' + post.url + '">';
              resultInnerHtml += post.title;
              resultInnerHtml += '</a><div class="search-results-content">';
              resultInnerHtml += post.content;
              resultInnerHtml += '</div></li>';
            });
            resultInnerHtml += '</ul>';
          } else {
            resultInnerHtml += '<div class="search-results-none"><i class="fa fa-meh-o"></i></div>';
          }

          $result.html(resultInnerHtml);
        };

        $input.on('input', searchPost);
        $input.on('keypress', function (e) {
          if (e.keyCode === Stun.utils.codeToKeyCode('Enter')) {
            searchPost();
          }
        });
      }
    });
  }

  function closeSearch () {
    $('body').css('overflow', 'auto');
    $('.search-popup')
      .removeClass('show')
      .velocity('stop')
      .velocity('transition.expandOut', {
        duration: 300
      });
    $('.search-mask')
      .velocity('stop')
      .velocity('transition.fadeOut', {
        duration: 300
      });
  }
}, false);</script><script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@latest/bsz.pure.mini.js" async></script><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-md5@latest/src/md5.min.js"></script><script>function loadGitalk () {
  if (!document.getElementById('gitalk-container')) {
    return;
  }

  var gitalk = new Gitalk({
    id: md5(window.location.pathname.slice(1)),
    clientID: '23d7b4a6c1b1589b9d9b',
    clientSecret: 'fcbbb62bf5fe3fac81979e69fd71cd637e0c523f',
    repo: 'Lvshaomei.github.io',
    owner: 'Lvshaomei',
    admin: ['Lvshaomei'],
    distractionFreeMode: 'true',
    language: 'zh-CN'
  });

  gitalk.render('gitalk-container');
}

if (false) {
  loadGitalk();
} else {
  window.addEventListener('DOMContentLoaded', loadGitalk, false);
}</script><script src="../../../../js/utils.js?v=1.5.4"></script><script src="../../../../js/stun-boot.js?v=1.5.4"></script><script src="../../../../js/scroll.js?v=1.5.4"></script><script src="../../../../js/header.js?v=1.5.4"></script><script src="../../../../js/sidebar.js?v=1.5.4"></script></body></html>