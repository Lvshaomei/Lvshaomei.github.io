<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="keywords" content="LSM, Mia's blog"><meta name="description" content="生活总是需要不断的跌倒，再不断的爬起来，这就是生活。"><title>【第一期AI夏令营丨自然语言处理】使用BERT模型解决问题 | Mia's blog</title><meta name="google-site-verification" content="HPcpCEPE_TQ7LlH-LAToYJleDQ7ydr0Mgiw95TQ5GNQ"><meta name="msvalidate.01" content="9C903B945C2FE1FB1A111163F0C32703"><meta name="theme-color" content="#54bcff"><link rel="manifest" href="../../../../manifest.json?v=1.5.4"><link rel="icon" href="../../../../images/icons/Lv-16.png?v=1.5.4" type="image/png" sizes="16x16"><link rel="icon" href="../../../../images/icons/Lv-32.png?v=1.5.4" type="image/png" sizes="32x32"><link rel="alternate" href="../../../../atom.xml" type="application/atom+xml"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" type="text/css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.css" type="text/css"><link rel="stylesheet" href="../../../../css/index.css?v=1.5.4"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="dns-prefetch" href="https://hm.baidu.com"><link rel="dns-prefetch" href="https://tajs.qq.com"><script src="https://www.googletagmanager.com/gtag/js?id=UA-156048106-1" async=""></script><script>if (window.location.hostname !== 'localhost') {
  window.dataLayer = window.dataLayer || [];
  function gtag(){ dataLayer.push(arguments); }
  gtag('js', new Date());
  gtag('config', 'UA-156048106-1');
}</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement('script');
  hm.src = 'https://hm.baidu.com/hm.js?3df607e8ce8da7ad737b9205f02966ee';
  hm.async = true;

  if (false) { hm.setAttribute('data-pjax', ''); }

  var s = document.getElementsByTagName('script')[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>(function() {
  var hm = document.createElement('script');
  hm.src = 'https://tajs.qq.com/stats?sId=undefined';
  hm.async = true;

  if (false) { hm.setAttribute('data-pjax', ''); }

  var s = document.getElementsByTagName('script')[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  fontawesome: {"prefix":"fa"},
  sidebar: {"offsetTop":"30px","renderTocDepth":4},
  header: {"scrollDownIcon":true},
  back2top: {"enable":true},
  reward: false,
  fancybox: false,
  zoom_image: {"enable":true,"mask_color":"rgba(0,0,0,0.6)"},
  gallery_waterfall: undefined,
  lazyload: undefined,
  pjax: undefined,
  external_link: {"icon":{"enable":true,"name":"external-link"}},
  shortcuts: {"switch_post":false},
  prompt: {"copy_success":"复制成功","copy_error":"复制失败","creative_commons":"知识共享","copy_button":"点击复制"}
};

window.CONFIG = CONFIG;</script></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav"><div class="header-nav-inner"><div class="header-nav-btn fa fa-bars"></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/"><i class="fa fa-home"></i>首页</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="javascript:;" target="_blank" rel="noopener"><i class="fa fa-paint-brush"></i>文章</a><div class="header-nav-submenu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/archives/"><i class="fa fa-folder-open"></i>归档</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/categories/"><i class="fa fa-th"></i>分类</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/tags/"><i class="fa fa-tags"></i>标签</a></div></div></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/about/"><i class="fa fa-user"></i>关于</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/friends/"><i class="fa fa-hand-spock-o"></i>友链</a></div></div><div class="header-nav-search"><i class="fa fa-search"></i><span>搜索</span></div></div></nav><div class="header-info"><div class="header-info-inner"><div class="header-info-title">Mia's blog</div><div class="header-info-subtitle">留意身边的美好</div></div><div class="header-info-scrolldown"><i class="fa fa-angle-down header-info-scrolldown__icon"></i></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content"><div class="post"><header class="post-header"><h1 class="post-header-title">【第一期AI夏令营丨自然语言处理】使用BERT模型解决问题</h1><div class="post-header-meta"><span class="post-header-meta-create"><i class="fa fa-calendar-o"></i><span>发表于 </span><span>2023-10-24</span></span><span class="post-header-meta-update"><i class="fa fa-calendar-check-o"></i><span>更新于 </span><span>2023-10-24</span></span><span class="post-header-meta-word-count"><i class="fa fa-file-word-o"></i><span>字数统计 </span><span>5.1k</span></span><span class="post-header-meta-reading-time"><i class="fa fa-clock-o"></i><span>阅读时长 </span><span>37m</span></span><span class="post-header-meta-reading-count"><i class="fa fa-eye"></i><span>阅读次数 </span><span id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body"><p>﻿# 一、使用预训练的BERT模型解决文本二分类问题</p>
<h2 id="深度学习模型训练的一般步骤："><span class="heading-link">深度学习模型训练的一般步骤：</span></h2><ol>
<li>导入前置依赖</li>
<li>设置全局配置</li>
<li>进行数据读取与数据预处理</li>
<li>构建训练所需的dataloader与dataset</li>
<li>定义预测模型</li>
<li>定义出损失函数和优化器</li>
<li>定义一个验证方法，获取到验证集的精准率和loss。</li>
<li>模型训练，保存最好的模型</li>
<li>加载最好的模型，然后进行测试集的预测</li>
<li>将测试数据送入模型，得到结果</li>
</ol>
<h2 id="1-导入前置依赖"><span class="heading-link">1. 导入前置依赖</span></h2><figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="comment"># 用于加载bert模型的分词器</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"><span class="comment"># 用于加载bert模型</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br></pre></td></tr></tbody></table></div></figure>

<p>当我们需要导入项目中的摸个函数时，应该这样操作：</p>
<blockquote>
<p>from 文件夹名.某个py文件 import 某个函数 </p>
</blockquote>
<p>例如在当前目录下有一个FaceModel文件夹，文件夹下有一个faceModel.py， py文件下有一个predict函数，那应该如何操作呢？ </p>
<figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span>  FaceModel.faceModel <span class="keyword">import</span> predict</span><br></pre></td></tr></tbody></table></div></figure>

<h2 id="2-设置全局配置"><span class="heading-link">2.设置全局配置</span></h2><p>主要设置一些超参数。超参数是在开 始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。</p>
<figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">16</span></span><br><span class="line"><span class="comment"># 文本的最大长度</span></span><br><span class="line">text_max_length = <span class="number">128</span></span><br><span class="line"><span class="comment"># 总训练的epochs数</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">lr = <span class="number">3e-5</span></span><br><span class="line"><span class="comment"># 取多少训练集的数据作为验证集</span></span><br><span class="line">validation_ratio = <span class="number">0.1</span></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每多少步，打印一次loss</span></span><br><span class="line">log_per_step = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集所在位置</span></span><br><span class="line">dataset_dir = Path(<span class="string">"./data"</span>)</span><br><span class="line">os.makedirs(dataset_dir) <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(dataset_dir) <span class="keyword">else</span> <span class="string">''</span> <span class="comment"># 当不存在这一文件夹时，就创建它</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型存储路径</span></span><br><span class="line">model_dir = Path(<span class="string">"./model/bert_checkpoints"</span>)</span><br><span class="line"><span class="comment"># 如果模型目录不存在，则创建一个</span></span><br><span class="line">os.makedirs(model_dir) <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(model_dir) <span class="keyword">else</span> <span class="string">''</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Device:"</span>, device)</span><br></pre></td></tr></tbody></table></div></figure>

<h2 id="3-进行数据读取与数据预处理"><span class="heading-link">3. 进行数据读取与数据预处理</span></h2><p>数据预处理的常见步骤：</p>
<ol>
<li><strong>数据清洗</strong>：检查数据中的缺失值、异常值、重复值等情况，并进行相应处理。可以使用插补方法填充缺失值，剔除异常值或者利用统计方法进行处理。</li>
<li><strong>特征选择</strong>：根据实际问题和领域知识，选择最相关和有用的特征。可以使用相关性分析、特征重要性评估等方法进行特征选择。</li>
<li><strong>特征缩放</strong>：将不同尺度或数量级的特征进行缩放，以保证模型的准确性和稳定性。常见的特征缩放方法包括标准化和归一化。</li>
<li><strong>特征编码</strong>：将非数值型的特征转换为数值型，以便模型可以进行处理。可以使用独热编码、标签编码等方法进行特征编码。</li>
<li><strong>数据集划分</strong>：将数据集划分为训练集、验证集和测试集。训练集用于模型训练，验证集用于模型调优和选择，测试集用于评估模型性能。</li>
<li><strong>处理类别不平衡</strong>：如果数据集中存在类别不平衡问题，可以采取一些方法来处理，例如欠采样、过采样等。</li>
</ol>
<p>具体的预处理方法和步骤会根据具体的数据和问题而有所不同。在实际应用中，根据具体情况选择适当的数据预处理方法非常重要，以提高模型的性能和准确性。</p>
<figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据集，进行数据处理</span></span><br><span class="line"></span><br><span class="line">pd_train_data = pd.read_csv(<span class="string">'./data/train.csv'</span>)</span><br><span class="line">pd_train_data[<span class="string">'title'</span>] = pd_train_data[<span class="string">'title'</span>].fillna(<span class="string">''</span>) <span class="comment"># 缺失值将被替换为空字符串</span></span><br><span class="line">pd_train_data[<span class="string">'abstract'</span>] = pd_train_data[<span class="string">'abstract'</span>].fillna(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">test_data = pd.read_csv(<span class="string">'./data/test.csv'</span>)</span><br><span class="line">test_data[<span class="string">'title'</span>] = test_data[<span class="string">'title'</span>].fillna(<span class="string">''</span>)</span><br><span class="line">test_data[<span class="string">'abstract'</span>] = test_data[<span class="string">'abstract'</span>].fillna(<span class="string">''</span>)</span><br><span class="line"><span class="comment"># 将几个字段连接在一起 形成一行文本数据</span></span><br><span class="line">pd_train_data[<span class="string">'text'</span>] = pd_train_data[<span class="string">'title'</span>].fillna(<span class="string">''</span>) + <span class="string">' '</span> +  pd_train_data[<span class="string">'author'</span>].fillna(<span class="string">''</span>) + <span class="string">' '</span> + pd_train_data[<span class="string">'abstract'</span>].fillna(<span class="string">''</span>)+ <span class="string">' '</span> + pd_train_data[<span class="string">'Keywords'</span>].fillna(<span class="string">''</span>)</span><br><span class="line">test_data[<span class="string">'text'</span>] = test_data[<span class="string">'title'</span>].fillna(<span class="string">''</span>) + <span class="string">' '</span> +  test_data[<span class="string">'author'</span>].fillna(<span class="string">''</span>) + <span class="string">' '</span> + test_data[<span class="string">'abstract'</span>].fillna(<span class="string">''</span>)+ <span class="string">' '</span> + pd_train_data[<span class="string">'Keywords'</span>].fillna(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从训练集中随机采样测试集</span></span><br><span class="line">validation_data = pd_train_data.sample(frac=validation_ratio)</span><br><span class="line">train_data = pd_train_data[~pd_train_data.index.isin(validation_data.index)]<span class="comment"># 获取不在验证集索引中的数据行</span></span><br></pre></td></tr></tbody></table></div></figure>

<p>构建数据集，将数据集划分为训练集、验证集和测试集</p>
<figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建Dataset</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mode=<span class="string">'train'</span>)</span>:</span></span><br><span class="line">        super(MyDataset, self).__init__()</span><br><span class="line">        self.mode = mode</span><br><span class="line">        <span class="comment"># 拿到对应的数据</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">            self.dataset = train_data</span><br><span class="line">        <span class="keyword">elif</span> mode == <span class="string">'validation'</span>:</span><br><span class="line">            self.dataset = validation_data</span><br><span class="line">        <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">            <span class="comment"># 如果是测试模式，则返回内容和uuid。拿uuid做target主要是方便后面写入结果。</span></span><br><span class="line">            self.dataset = test_data</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">"Unknown mode {}"</span>.format(mode))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="comment"># 取第index条</span></span><br><span class="line">        data = self.dataset.iloc[index]</span><br><span class="line">        <span class="comment"># 取其内容</span></span><br><span class="line">        text = data[<span class="string">'text'</span>]</span><br><span class="line">        <span class="comment"># 根据状态返回内容</span></span><br><span class="line">        <span class="keyword">if</span> self.mode == <span class="string">'test'</span>:</span><br><span class="line">            <span class="comment"># 如果是test，将uuid做为target</span></span><br><span class="line">            label = data[<span class="string">'uuid'</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            label = data[<span class="string">'label'</span>]</span><br><span class="line">        <span class="comment"># 返回内容和label</span></span><br><span class="line">        <span class="keyword">return</span> text, label</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.dataset)</span><br><span class="line"></span><br><span class="line">train_dataset = MyDataset(<span class="string">'train'</span>)</span><br><span class="line">validation_dataset = MyDataset(<span class="string">'validation'</span>)</span><br><span class="line"><span class="comment"># 获取Bert预训练模型</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">"bert-base-uncased"</span>) <span class="comment"># 使用Hugging Face库中的AutoTokenizer类来加载预训练的BERT模型的tokenizer</span></span><br></pre></td></tr></tbody></table></div></figure>

<h2 id="4-构建训练所需的dataloader与dataset"><span class="heading-link">4. 构建训练所需的dataloader与dataset</span></h2><p>接构造Dataloader，需要定义一下collate_fn，在其中完成对句子进行编码、填充、组装batch等动作：</p>
<figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(batch)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将一个batch的文本句子转成tensor，并组成batch。</span></span><br><span class="line"><span class="string">    :param batch: 一个batch的句子，例如: [('推文', target), ('推文', target), ...]</span></span><br><span class="line"><span class="string">    :return: 处理后的结果，例如：</span></span><br><span class="line"><span class="string">             src: {'input_ids': tensor([[ 101, ..., 102, 0, 0, ...], ...]), 'attention_mask': tensor([[1, ..., 1, 0, ...], ...])}</span></span><br><span class="line"><span class="string">             target：[1, 1, 0, ...]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    text, label = zip(*batch) <span class="comment"># 带有星号（*）作为前缀的参数表示可变长度的位置参数。</span></span><br><span class="line">    print(<span class="string">'text:'</span>,text, <span class="string">'label:'</span>,label)</span><br><span class="line">    text, label = list(text), list(label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># src是要送给bert的，所以不需要特殊处理，直接用tokenizer的结果即可    编码后的文本数据。</span></span><br><span class="line">    <span class="comment"># padding='max_length' 不够长度的进行填充</span></span><br><span class="line">    <span class="comment"># truncation=True 长度过长的进行裁剪</span></span><br><span class="line">    <span class="comment"># eturn_tensors=‘pt’: 指定返回PyTorch张量对象。</span></span><br><span class="line">    src = tokenizer(text, padding=<span class="string">'max_length'</span>, max_length=text_max_length, return_tensors=<span class="string">'pt'</span>, truncation=<span class="literal">True</span>)</span><br><span class="line">    print(<span class="string">'src:'</span>,src)</span><br><span class="line">    <span class="keyword">return</span> src, torch.LongTensor(label)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>, collate_fn=collate_fn)</span><br><span class="line">validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>, collate_fn=collate_fn)</span><br></pre></td></tr></tbody></table></div></figure>

<p><img src="/2023/10/24/%E3%80%90%E7%AC%AC%E4%B8%80%E6%9C%9FAI%E5%A4%8F%E4%BB%A4%E8%90%A5%E4%B8%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%91%E4%BD%BF%E7%94%A8BERT%E6%A8%A1%E5%9E%8B%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98/2.png" alt="image.png"><br><img src="/2023/10/24/%E3%80%90%E7%AC%AC%E4%B8%80%E6%9C%9FAI%E5%A4%8F%E4%BB%A4%E8%90%A5%E4%B8%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%91%E4%BD%BF%E7%94%A8BERT%E6%A8%A1%E5%9E%8B%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98/1.png" alt=""><br><strong>下面是对BERT模型的详细介绍：</strong></p>
<ol>
<li><strong>架构</strong>：BERT模型的核心是Transformer架构，它由多个编码器层组成。每个编码器层都由多头自注意力机制（Multi-Head Self-Attention）和前馈神经网络（Feed-Forward Neural Network）组成。</li>
<li><strong>预训练阶段</strong>：BERT在预训练阶段通过两个自监督任务来学习文本表示：Masked Language Model（MLM）和Next Sentence Prediction（NSP）。</li>
<li><strong>MLM</strong>：模型随机地遮盖输入文本的一部分单词，并训练来预测这些被遮盖的单词。这样可以使模型学会理解上下文和句子中的关系以及词汇的表征。</li>
<li><strong>NSP</strong>：模型输入两个句子，并判断这两个句子是否相邻。这个任务可以使模型学会理解句子级别的关系和上下文之间的相关性。</li>
<li><strong>微调阶段</strong>：在预训练阶段得到的BERT模型可以在特定的下游任务上进行微调。这些下游任务可能包括文本分类、命名实体识别、问答等。在微调阶段，BERT模型通过在下游任务上进行有监督学习来进一步优化和适应。</li>
<li><strong>输入表示</strong>：BERT模型的输入通常是经过分词（tokenization）后的文本。BERT使用WordPiece分词技术将输入序列拆分为多个子词（subword）。每个子词都有一个唯一的标记，并且可以通过词嵌入得到对应的向量表示。</li>
<li><strong>输出表示</strong>：BERT模型在每一层的输出都包含了每个输入的表示。通常情况下，我们只使用最后一层的输出作为输入文本的表示，也可以使用多层的输出进行组合。</li>
<li><strong>上下文无关性和上下文敏感性</strong>：BERT模型通过上下文无关的方式进行预训练。这意味着模型可以独立地对每个输入进行编码，而不考虑其上下文信息。在微调和应用阶段，BERT模型可以根据需要进行上下文敏感性编码。</li>
</ol>
<p><strong>BERT模型的优点</strong>是能够学习到更好的语言表示，能够根据上下文理解词汇的含义和句子的关系，并在各种下游任务上取得了良好的性能。但它也有一些限制，例如计算资源要求较高，模型较大，需要较长的训练时间。</p>
<h2 id="5-定义预测模型"><span class="heading-link">5. 定义预测模型</span></h2><figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MyModel, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加载bert模型</span></span><br><span class="line">        self.bert = BertModel.from_pretrained(<span class="string">'bert-base-uncased'</span>, mirror=<span class="string">'tuna'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最后的预测层</span></span><br><span class="line">        self.predictor = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">768</span>, <span class="number">256</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">1</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param src: 分词后的推文数据</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将src直接序列解包传入bert，因为bert和tokenizer是一套的，所以可以这么做。</span></span><br><span class="line">        <span class="comment"># 得到encoder的输出，用最前面[CLS]的输出作为最终线性层的输入</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ".last_hidden_state[:, 0, :]“的操作，从模型的最后一个隐藏状态中提取有用的信息</span></span><br><span class="line">        <span class="comment"># 使用切片操作”[:, 0, :]"，即保留所有样本的第0个位置的隐藏状态特征，</span></span><br><span class="line">        <span class="comment">#对应于BERT模型的CLS（Classification）标记。这个CLS特征通常被用作文本分类或序列标注任务的整体表示。</span></span><br><span class="line">        outputs = self.bert(**src).last_hidden_state[:, <span class="number">0</span>, :] </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用线性层来做最终的预测</span></span><br><span class="line">        <span class="keyword">return</span> self.predictor(outputs)</span><br><span class="line"></span><br><span class="line">model = MyModel()</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></tbody></table></div></figure>

<p>BERT模型在文本前插入一个[CLS]符号，并将该符号对应的输出向量作为整篇文本的语义表示，用于文本分类，如下图所示。可以理解为：与文本中已有的其它字/词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个字/词的语义信息。</p>
<h2 id="6-定义出损失函数和优化器"><span class="heading-link">6. 定义出损失函数和优化器</span></h2><p><strong>二元交叉熵（Binary Cross Entropy）</strong>是一种用于衡量两个概率分布之间差异的损失函数，通常用于二分类问题。<br><img src="/2023/10/24/%E3%80%90%E7%AC%AC%E4%B8%80%E6%9C%9FAI%E5%A4%8F%E4%BB%A4%E8%90%A5%E4%B8%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%91%E4%BD%BF%E7%94%A8BERT%E6%A8%A1%E5%9E%8B%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98/2690707a80bb483fa77edc1eec5ad60a.png" alt="在这里插入图片描述"><br>其中，L表示损失，y是真实标签（取值为0或1），p是模型输出的概率（预测为类别1的概率）。当y为1时，损失函数的第一项起作用，计算的是模型正确预测为类别1的概率的对数。当y为0时，损失函数的第二项起作用，计算的是模型正确预测为类别0的概率的对数。<br>通过最小化二元交叉熵损失函数，我们可以使得模型对两个类别的分类更加准确。</p>
<figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">criteria = nn.BCELoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于inputs是字典类型的，定义一个辅助函数帮助to(device)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_device</span><span class="params">(dict_tensors)</span>:</span></span><br><span class="line">    result_tensors = {}</span><br><span class="line">    <span class="keyword">for</span> key, value <span class="keyword">in</span> dict_tensors.items():</span><br><span class="line">        result_tensors[key] = value.to(device) <span class="comment"># 将张量移动到指定的设备上</span></span><br><span class="line">    <span class="keyword">return</span> result_tensors</span><br></pre></td></tr></tbody></table></div></figure>

<h2 id="7-定义一个验证方法，获取到验证集的精准率和loss"><span class="heading-link">7. 定义一个验证方法，获取到验证集的精准率和loss</span></h2><figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validate</span><span class="params">()</span>:</span></span><br><span class="line">    model.eval() <span class="comment"># 将模型切换到评估模式</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    total_correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> inputs, targets <span class="keyword">in</span> validation_loader:</span><br><span class="line">        inputs, targets = to_device(inputs), targets.to(device)</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        <span class="comment"># view(-1) 可以自动计算其他维度的大小</span></span><br><span class="line">        loss = criteria(outputs.view(<span class="number">-1</span>), targets.float()) <span class="comment"># outputs.view(-1) 意味着它会将原始张量中的所有元素展平，并将它们放置在一个单一的维度中</span></span><br><span class="line">        total_loss += float(loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算模型的预测结果与目标数据之间的正确预测数量</span></span><br><span class="line">        correct_num = (((outputs &gt;= <span class="number">0.5</span>).float() * <span class="number">1</span>).flatten() == targets).sum()</span><br><span class="line">        total_correct += correct_num <span class="comment"># 累加每个批次的正确预测数量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_correct / len(validation_dataset), total_loss / len(validation_dataset)</span><br></pre></td></tr></tbody></table></div></figure>

<h2 id="8-模型训练，保存最好的模型"><span class="heading-link">8. 模型训练，保存最好的模型</span></h2><figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先将模型调成训练模式</span></span><br><span class="line">model.train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清空一下cuda缓存</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义几个变量，帮助打印loss</span></span><br><span class="line">total_loss = <span class="number">0.</span></span><br><span class="line"><span class="comment"># 记录步数</span></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 记录在验证集上最好的准确率</span></span><br><span class="line">best_accuracy = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> i, (inputs, targets) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        <span class="comment"># 从batch中拿到训练数据</span></span><br><span class="line">        inputs, targets = to_device(inputs), targets.to(device)</span><br><span class="line">        <span class="comment"># 传入模型进行前向传递</span></span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = criteria(outputs.view(<span class="number">-1</span>), targets.float())</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        total_loss += float(loss)</span><br><span class="line">        step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % log_per_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch {}/{}, Step: {}/{}, total loss:{:.4f}"</span>.format(epoch+<span class="number">1</span>, epochs, i, len(train_loader), total_loss))</span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">del</span> inputs, targets</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 一个epoch后，使用过验证集进行验证</span></span><br><span class="line">    accuracy, validation_loss = validate()</span><br><span class="line">    print(<span class="string">"Epoch {}, accuracy: {:.4f}, validation loss: {:.4f}"</span>.format(epoch+<span class="number">1</span>, accuracy, validation_loss))</span><br><span class="line">    torch.save(model, model_dir / <span class="string">f"model_<span class="subst">{epoch}</span>.pt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存最好的模型</span></span><br><span class="line">    <span class="keyword">if</span> accuracy &gt; best_accuracy:</span><br><span class="line">        torch.save(model, model_dir / <span class="string">f"model_best.pt"</span>)</span><br><span class="line">        best_accuracy = accuracy</span><br></pre></td></tr></tbody></table></div></figure>

<h2 id="9-加载最好的模型，然后进行测试集的预测"><span class="heading-link">9. 加载最好的模型，然后进行测试集的预测</span></h2><figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = torch.load(model_dir / <span class="string">f"model_best.pt"</span>)</span><br><span class="line">model = model.eval()</span><br><span class="line"></span><br><span class="line">test_dataset = MyDataset(<span class="string">'test'</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>, collate_fn=collate_fn)</span><br></pre></td></tr></tbody></table></div></figure>

<h2 id="10-将测试数据送入模型，得到结果"><span class="heading-link">10. 将测试数据送入模型，得到结果</span></h2><figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> inputs, ids <span class="keyword">in</span> test_loader:</span><br><span class="line">    outputs = model(inputs.to(device))</span><br><span class="line">    outputs = (outputs &gt;= <span class="number">0.5</span>).int().flatten().tolist()</span><br><span class="line">    ids = ids.tolist()</span><br><span class="line">    results = results + [(id, result) <span class="keyword">for</span> result, id <span class="keyword">in</span> zip(outputs, ids)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test_label = [pair[<span class="number">1</span>] <span class="keyword">for</span> pair <span class="keyword">in</span> results]</span><br><span class="line">test_data[<span class="string">'label'</span>] = test_label</span><br><span class="line">test_data[[<span class="string">'uuid'</span>, <span class="string">'label'</span>]].to_csv(<span class="string">'submit_task1_test.csv'</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></div></figure>

<h1 id="二、Bert-for-关键词提取"><span class="heading-link">二、Bert_for_关键词提取</span></h1><h2 id="1-导入前置依赖-1"><span class="heading-link">1. 导入前置依赖</span></h2><figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入pandas用于读取表格数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入BOW（词袋模型），可以选择将CountVectorizer替换为TfidfVectorizer（TF-IDF（词频-逆文档频率）），注意上下文要同时修改，亲测后者效果更佳</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="comment"># 导入Bert模型</span></span><br><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入计算相似度前置库，为了计算候选者和文档之间的相似度，我们将使用向量之间的余弦相似度，因为它在高维度下表现得相当好。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line"></span><br><span class="line"><span class="comment"># 过滤警告消息</span></span><br><span class="line"><span class="keyword">from</span> warnings <span class="keyword">import</span> simplefilter</span><br><span class="line"><span class="keyword">from</span> sklearn.exceptions <span class="keyword">import</span> ConvergenceWarning</span><br><span class="line">simplefilter(<span class="string">"ignore"</span>, category=ConvergenceWarning)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">texts=[<span class="string">"dog cat fish"</span>,<span class="string">"dog cat cat"</span>,<span class="string">"fish bird"</span>, <span class="string">'bird'</span>] <span class="comment"># “dog cat fish” 为输入列表元素,即代表一个文章的字符串</span></span><br><span class="line">cv = TfidfVectorizer()<span class="comment">#创建词袋数据结构</span></span><br><span class="line">cv_fit=cv.fit_transform(texts)</span><br><span class="line"><span class="comment">#上述代码等价于下面两行</span></span><br><span class="line"><span class="comment">#cv.fit(texts)</span></span><br><span class="line"><span class="comment">#cv_fit=cv.transform(texts)</span></span><br><span class="line"></span><br><span class="line">print(cv.get_feature_names())    <span class="comment">#['bird', 'cat', 'dog', 'fish'] 列表形式呈现文章生成的词典</span></span><br><span class="line"></span><br><span class="line">print(cv.vocabulary_	)              <span class="comment"># {‘dog’:2,'cat':1,'fish':3,'bird':0} 字典形式呈现，key：词，value:词频</span></span><br><span class="line"></span><br><span class="line">print(cv_fit)      <span class="comment"># 第n个列表元素，**词典中索引为n的元素**， 词频</span></span><br></pre></td></tr></tbody></table></div></figure>

<h2 id="2-读取数据集并处理"><span class="heading-link">2. 读取数据集并处理</span></h2><figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据集</span></span><br><span class="line">test = pd.read_csv(<span class="string">'./data/testB.csv'</span>)</span><br><span class="line">test[<span class="string">'title'</span>] = test[<span class="string">'title'</span>].fillna(<span class="string">''</span>)</span><br><span class="line">test[<span class="string">'abstract'</span>] = test[<span class="string">'abstract'</span>].fillna(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test[<span class="string">'text'</span>] = test[<span class="string">'title'</span>].fillna(<span class="string">''</span>) + <span class="string">' '</span> +test[<span class="string">'abstract'</span>].fillna(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义停用词，去掉出现较多，但对文章不关键的词语</span></span><br><span class="line">stops =[i.strip() <span class="keyword">for</span> i <span class="keyword">in</span> open(<span class="string">r'stop.txt'</span>,encoding=<span class="string">'utf-8'</span>).readlines()]</span><br></pre></td></tr></tbody></table></div></figure>

<p>使用n_gram_range来改变结果候选词的词长大小。例如，如果我们将它设置为(3，3)，那么产生的候选词将是包含3个关键词的短语。然后，变量candidates就是一个简单的字符串列表，其中包含了我们的候选关键词或者关键短语。</p>
<h2 id="3-Embeddings"><span class="heading-link">3. Embeddings</span></h2><figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model = SentenceTransformer(<span class="string">r'xlm-r-distilroberta-base-paraphrase-v1'</span>)</span><br><span class="line"></span><br><span class="line">test_words = []</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> test.iterrows():</span><br><span class="line">    <span class="comment"># 读取第每一行数据的标题与摘要并提取关键词</span></span><br><span class="line">    </span><br><span class="line">    n_gram_range = (<span class="number">2</span>,<span class="number">2</span>) <span class="comment">#要考虑的 n-gram 的范围是 2-gram 到 2-gram，也就是只考虑连续两个词组成的序列。</span></span><br><span class="line">    <span class="comment"># 这里我们使用TF-IDF算法来获取候选关键词  </span></span><br><span class="line">    count = TfidfVectorizer(ngram_range=n_gram_range, stop_words=stops).fit([row[<span class="number">1</span>].text]) <span class="comment"># 从一个文本数据集中创建了一个 TF-IDF 特征计数器（feature counter）。</span></span><br><span class="line">    candidates = count.get_feature_names()</span><br><span class="line">    print(candidates)</span><br><span class="line">    <span class="comment"># 将文本标题以及候选关键词/关键短语转换为数值型数据（numerical data）。我们使用BERT来实现这一目的</span></span><br><span class="line">    title_embedding = model.encode([row[<span class="number">1</span>].title])</span><br><span class="line">    </span><br><span class="line">    candidate_embeddings = model.encode(candidates)</span><br></pre></td></tr></tbody></table></div></figure>

<h2 id="4-Cosine-Similarity"><span class="heading-link">4. Cosine Similarity</span></h2><p>要找到与文档最相似的候选词汇或者短语。假设与文档最相似的候选词汇/短语，是能较好的表示文档的关键词/关键短语。为了计算候选者和文档之间的相似度，将使用向量之间的余弦相似度，因为它在高维度下表现得相当好。</p>
<figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过修改这个参数来更改关键词数量</span></span><br><span class="line">top_n = <span class="number">35</span></span><br><span class="line"><span class="comment"># 利用文章标题进一步提取关键词</span></span><br><span class="line">distances = cosine_similarity(title_embedding, candidate_embeddings)</span><br><span class="line">keywords = [candidates[index] <span class="keyword">for</span> index <span class="keyword">in</span> distances.argsort()[<span class="number">0</span>][-top_n:]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> len( keywords) == <span class="number">0</span>:</span><br><span class="line">     keywords = [<span class="string">'A'</span>, <span class="string">'B'</span>]</span><br><span class="line">test_words.append(<span class="string">'; '</span>.join( keywords))</span><br></pre></td></tr></tbody></table></div></figure>

<figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">print(keywords)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">输出结果：</span></span><br><span class="line"><span class="string">['monomers roasting', 'ara monomers', 'enzyme linked', 'degranulation basophils', </span></span><br><span class="line"><span class="string">'matrix amount', 'total proteins', 'stimulate degranulation', 'roasting ara', </span></span><br><span class="line"><span class="string">'allergenicity increase', 'structure ara', 'allergenicity cross', 'allergenicity change',</span></span><br><span class="line"><span class="string">'proteins iac', 'addition methylation', 'processing roasting', 'food allergy', </span></span><br><span class="line"><span class="string">'derivatives roasting', 'ara roasted', 'ara matrix', 'processing structure', </span></span><br><span class="line"><span class="string">'reflect allergenicity', 'oxidation modification', 'allergenicity ara', </span></span><br><span class="line"><span class="string">'blotting enzyme', 'reduce allergenicity', 'potential allergenicity', </span></span><br><span class="line"><span class="string">'terms allergenicity', 'roasted matrix', 'peanut allergy', 'matrix peanut', </span></span><br><span class="line"><span class="string">'methylation oxidation', 'structure allergenicity', 'allergenicity processing', </span></span><br><span class="line"><span class="string">'allergenicity peanut', 'peanut allergen']</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></tbody></table></div></figure>

<p>所有的关键词/短语都是如此的相似，所以可以考虑结果的多样化策略。</p>
<h2 id="5-Diversification"><span class="heading-link">5. Diversification</span></h2><p>结果的多样化需要在关键词/关键短语的准确性（accuracy）和它们之间的多样性（diversity）之间取得一个微妙的平衡（a delicate balance）。使用两种算法来实现结果的多样化。<strong>可参考：</strong><span class="external-link"><a href="https://www.heywhale.com/mw/project/5fe7457e5e24ed0030239a11" target="_blank" rel="noopener"><strong>基于上下文语境的文档关键词提取</strong></a><i class="fa fa-external-link"></i></span></p>
<ul>
<li>Max Sum Similarity（最大相似度）</li>
<li>Maximal Marginal Relevance（最大边际相关性）</li>
</ul>
<h3 id="5-1-Max-Sum-Similarity（最大相似度）"><span class="heading-link">5.1  Max Sum Similarity（最大相似度）</span></h3><figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_sum_sim</span><span class="params">(doc_embedding, word_embeddings, words, top_n, nr_candidates)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    使用余弦相似度计算文档嵌入向量和候选词嵌入向量之间的相似度。</span></span><br><span class="line"><span class="string">	根据余弦相似度的值，选择具有最高相似度的候选词作为候选集合。</span></span><br><span class="line"><span class="string">	构建候选词之间相似度的矩阵。</span></span><br><span class="line"><span class="string">	使用贪心算法，选择使得相似度之和最大化的词组合作为最终的多样化结果。</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># Calculate distances and extract keywords</span></span><br><span class="line">    distances = cosine_similarity(doc_embedding, candidate_embeddings)</span><br><span class="line">    distances_candidates = cosine_similarity(candidate_embeddings, </span><br><span class="line">                                            candidate_embeddings)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get top_n words as candidates based on cosine similarity</span></span><br><span class="line">    words_idx = list(distances.argsort()[<span class="number">0</span>][-nr_candidates:])</span><br><span class="line">    words_vals = [candidates[index] <span class="keyword">for</span> index <span class="keyword">in</span> words_idx]</span><br><span class="line">    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the combination of words that are the least similar to each other</span></span><br><span class="line">    min_sim = np.inf</span><br><span class="line">    candidate = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> combination <span class="keyword">in</span> itertools.combinations(range(len(words_idx)), top_n):</span><br><span class="line">        sim = sum([distances_candidates[i][j] <span class="keyword">for</span> i <span class="keyword">in</span> combination <span class="keyword">for</span> j <span class="keyword">in</span> combination <span class="keyword">if</span> i != j])</span><br><span class="line">        <span class="keyword">if</span> sim &lt; min_sim:</span><br><span class="line">            candidate = combination</span><br><span class="line">            min_sim = sim</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [words_vals[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> candidate]</span><br></pre></td></tr></tbody></table></div></figure>

<figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">max_sum_sim(doc_embedding=title_embedding, </span><br><span class="line">            word_embeddings=candidate_embeddings, </span><br><span class="line">            words=candidates, </span><br><span class="line">            top_n=<span class="number">10</span>, </span><br><span class="line">            nr_candidates=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">输出结果：</span></span><br><span class="line"><span class="string">['potential allergenicity',</span></span><br><span class="line"><span class="string"> 'terms allergenicity',</span></span><br><span class="line"><span class="string"> 'roasted matrix',</span></span><br><span class="line"><span class="string"> 'peanut allergy',</span></span><br><span class="line"><span class="string"> 'matrix peanut',</span></span><br><span class="line"><span class="string"> 'methylation oxidation',</span></span><br><span class="line"><span class="string"> 'structure allergenicity',</span></span><br><span class="line"><span class="string"> 'allergenicity processing',</span></span><br><span class="line"><span class="string"> 'allergenicity peanut',</span></span><br><span class="line"><span class="string"> 'peanut allergen']</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">max_sum_sim(doc_embedding=title_embedding, </span><br><span class="line">            word_embeddings=candidate_embeddings, </span><br><span class="line">            words=candidates, </span><br><span class="line">            top_n=<span class="number">10</span>, </span><br><span class="line">            nr_candidates=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">输出结果：</span></span><br><span class="line"><span class="string">['derivatives roasting',</span></span><br><span class="line"><span class="string"> 'ara roasted',</span></span><br><span class="line"><span class="string"> 'ara matrix',</span></span><br><span class="line"><span class="string"> 'processing structure',</span></span><br><span class="line"><span class="string"> 'oxidation modification',</span></span><br><span class="line"><span class="string"> 'reduce allergenicity',</span></span><br><span class="line"><span class="string"> 'potential allergenicity',</span></span><br><span class="line"><span class="string"> 'peanut allergy',</span></span><br><span class="line"><span class="string"> 'matrix peanut',</span></span><br><span class="line"><span class="string"> 'allergenicity processing']</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></tbody></table></div></figure>

<p><strong>较高的nr_candidates值会创造出更多样化的关键词/关键短语，但这并不能很好地代表文档。</strong></p>
<figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">test_words = []</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> test.iterrows():</span><br><span class="line">    <span class="comment"># 读取第每一行数据的标题与摘要并提取关键词</span></span><br><span class="line">    </span><br><span class="line">    n_gram_range = (<span class="number">2</span>,<span class="number">2</span>) <span class="comment">#要考虑的 n-gram 的范围是 2-gram 到 2-gram，也就是只考虑连续两个词组成的序列。</span></span><br><span class="line">    <span class="comment"># 这里我们使用TF-IDF算法来获取候选关键词  </span></span><br><span class="line">    count = TfidfVectorizer(ngram_range=n_gram_range, stop_words=stops).fit([row[<span class="number">1</span>].text]) <span class="comment"># 从一个文本数据集中创建了一个 TF-IDF 特征计数器（feature counter）。</span></span><br><span class="line">    candidates = count.get_feature_names()</span><br><span class="line">    print(candidates)</span><br><span class="line">    <span class="comment"># 将文本标题以及候选关键词/关键短语转换为数值型数据（numerical data）。我们使用BERT来实现这一目的</span></span><br><span class="line">    title_embedding = model.encode([row[<span class="number">1</span>].title])</span><br><span class="line">    </span><br><span class="line">    candidate_embeddings = model.encode(candidates)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 通过修改这个参数来更改关键词数量</span></span><br><span class="line">    top_n = <span class="number">35</span></span><br><span class="line">    <span class="comment"># 利用文章标题进一步提取关键词.</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    keywords = max_sum_sim(doc_embedding=title_embedding, </span><br><span class="line">            word_embeddings=candidate_embeddings, </span><br><span class="line">            words=candidates, </span><br><span class="line">            top_n=<span class="number">10</span>, </span><br><span class="line">            nr_candidates=<span class="number">10</span>)</span><br><span class="line">     <span class="comment">###########################################################################</span></span><br><span class="line"><span class="comment">#     distances = cosine_similarity(title_embedding, candidate_embeddings)</span></span><br><span class="line"><span class="comment">#     keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> len( keywords) == <span class="number">0</span>:</span><br><span class="line">         keywords = [<span class="string">'A'</span>, <span class="string">'B'</span>]</span><br><span class="line">    test_words.append(<span class="string">'; '</span>.join( keywords))</span><br></pre></td></tr></tbody></table></div></figure>

<h3 id="5-2-Maximal-Marginal-Relevance（最大边际相关性）"><span class="heading-link">5.2 Maximal Marginal Relevance（最大边际相关性）</span></h3><p><strong>最大边际相关性试图在文本摘要任务中最小化冗余（minimize redundancy）和最大化结果的多样性。</strong></p>
<figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mmr</span><span class="params">(doc_embedding, word_embeddings, words, top_n, diversity)</span>:</span></span><br><span class="line">	<span class="string">'''</span></span><br><span class="line"><span class="string">     使用余弦相似度计算每个候选词与文档嵌入向量的相似度以及候选词之间的相似度。</span></span><br><span class="line"><span class="string">	初始化已选择的关键词列表，首先选择与文档嵌入向量相似度最高的候选词作为第一个关键词。</span></span><br><span class="line"><span class="string">	根据要选择的关键词数量 top_n 进行循环迭代，每次选择与已选择关键词之间边际相关性最大的候选词作为下一个关键词。</span></span><br><span class="line"><span class="string">	更新已选择的关键词列表和候选词列表。</span></span><br><span class="line"><span class="string"> 	'''</span></span><br><span class="line">    <span class="comment"># Extract similarity within words, and between words and the document</span></span><br><span class="line">    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)</span><br><span class="line">    word_similarity = cosine_similarity(word_embeddings)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize candidates and already choose best keyword/keyphras</span></span><br><span class="line">    keywords_idx = [np.argmax(word_doc_similarity)]</span><br><span class="line">    candidates_idx = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(len(words)) <span class="keyword">if</span> i != keywords_idx[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(top_n - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># Extract similarities within candidates and</span></span><br><span class="line">        <span class="comment"># between candidates and selected keywords/phrases</span></span><br><span class="line">        candidate_similarities = word_doc_similarity[candidates_idx, :]</span><br><span class="line">        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate MMR</span></span><br><span class="line">        mmr = (<span class="number">1</span>-diversity) * candidate_similarities - diversity * target_similarities.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        mmr_idx = candidates_idx[np.argmax(mmr)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update keywords &amp; candidates</span></span><br><span class="line">        keywords_idx.append(mmr_idx)</span><br><span class="line">        candidates_idx.remove(mmr_idx)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [words[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> keywords_idx]</span><br></pre></td></tr></tbody></table></div></figure>

<figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mmr(doc_embedding=title_embedding, </span><br><span class="line">    word_embeddings=candidate_embeddings,</span><br><span class="line">    words=candidates, </span><br><span class="line">    top_n=<span class="number">20</span>, </span><br><span class="line">    diversity=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">mmr(doc_embedding=title_embedding, </span><br><span class="line">    word_embeddings=candidate_embeddings,</span><br><span class="line">    words=candidates, </span><br><span class="line">    top_n=<span class="number">20</span>, </span><br><span class="line">    diversity=<span class="number">0.8</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">''''</span></span><br><span class="line"><span class="string">同样的，较高的多样性数值会生成非常多样化的关键词/关键短语</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></tbody></table></div></figure>

<h1 id="参考"><span class="heading-link">参考</span></h1><p>[1]<span class="external-link"><a href="https://datawhaler.feishu.cn/docx/EVoodR6WroWZxXxa3a0cukIanRO" target="_blank" rel="noopener"> AI夏令营 - NLP实践教程</a><i class="fa fa-external-link"></i></span><br>[2] <span class="external-link"><a href="https://www.heywhale.com/mw/project/5fe7457e5e24ed0030239a11" target="_blank" rel="noopener">基于上下文语境的文档关键词提取</a><i class="fa fa-external-link"></i></span></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><footer class="post-footer"><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-author-name">本文作者: </span><span class="post-copyright-author-value"><a href="https://lvshaomei.github.io">LSM</a></span></div><div class="post-copyright-link"><span class="post-copyright-link-name">本文链接: </span><span class="post-copyright-link-value"><a href="https://lvshaomei.github.io/2023/10/24/%E3%80%90%E7%AC%AC%E4%B8%80%E6%9C%9FAI%E5%A4%8F%E4%BB%A4%E8%90%A5%E4%B8%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%91%E4%BD%BF%E7%94%A8BERT%E6%A8%A1%E5%9E%8B%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98/">https://lvshaomei.github.io/2023/10/24/%E3%80%90%E7%AC%AC%E4%B8%80%E6%9C%9FAI%E5%A4%8F%E4%BB%A4%E8%90%A5%E4%B8%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%91%E4%BD%BF%E7%94%A8BERT%E6%A8%A1%E5%9E%8B%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-notice-name">版权声明: </span><span class="post-copyright-notice-value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-cn" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><nav class="paginator"><div class="paginator-post"><div class="paginator-post-prev"><a class="paginator-post-prev__a" href="../%E5%A4%9A%E5%8A%9F%E8%83%BDstreamlit-App/"><i class="fa fa-chevron-left"></i><span>多功能streamlit_App</span></a></div><div class="paginator-post-next"><a class="paginator-post-next__a" href="../%E3%80%90%E7%AC%AC%E4%B8%80%E6%9C%9FAI%E5%A4%8F%E4%BB%A4%E8%90%A5%E4%B8%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%91%E4%BB%BB%E5%8A%A1%E4%B8%80%EF%BC%9A%E6%96%87%E6%9C%AC%E4%BA%8C%E5%88%86%E7%B1%BB-Baseline%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90%E5%8F%8A%E6%94%B9%E8%BF%9B/"><span>【第一期AI夏令营丨自然语言处理】任务一：文本二分类_&amp;&amp;Baseline代码分析及改进</span><i class="fa fa-chevron-right"></i></a></div></div></nav></footer></div></div><div class="comments" id="comments"><div id="gitalk-container"></div></div></div><aside class="sidebar" id="sidebar"><div class="sidebar-inner"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#深度学习模型训练的一般步骤："><span class="toc-number">1.</span> <span class="toc-text">深度学习模型训练的一般步骤：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-导入前置依赖"><span class="toc-number">2.</span> <span class="toc-text">1. 导入前置依赖</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-设置全局配置"><span class="toc-number">3.</span> <span class="toc-text">2.设置全局配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-进行数据读取与数据预处理"><span class="toc-number">4.</span> <span class="toc-text">3. 进行数据读取与数据预处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-构建训练所需的dataloader与dataset"><span class="toc-number">5.</span> <span class="toc-text">4. 构建训练所需的dataloader与dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-定义预测模型"><span class="toc-number">6.</span> <span class="toc-text">5. 定义预测模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-定义出损失函数和优化器"><span class="toc-number">7.</span> <span class="toc-text">6. 定义出损失函数和优化器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-定义一个验证方法，获取到验证集的精准率和loss"><span class="toc-number">8.</span> <span class="toc-text">7. 定义一个验证方法，获取到验证集的精准率和loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-模型训练，保存最好的模型"><span class="toc-number">9.</span> <span class="toc-text">8. 模型训练，保存最好的模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-加载最好的模型，然后进行测试集的预测"><span class="toc-number">10.</span> <span class="toc-text">9. 加载最好的模型，然后进行测试集的预测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-将测试数据送入模型，得到结果"><span class="toc-number">11.</span> <span class="toc-text">10. 将测试数据送入模型，得到结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二、Bert-for-关键词提取"><span class="toc-number"></span> <span class="toc-text">二、Bert_for_关键词提取</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-导入前置依赖-1"><span class="toc-number">1.</span> <span class="toc-text">1. 导入前置依赖</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-读取数据集并处理"><span class="toc-number">2.</span> <span class="toc-text">2. 读取数据集并处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Embeddings"><span class="toc-number">3.</span> <span class="toc-text">3. Embeddings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Cosine-Similarity"><span class="toc-number">4.</span> <span class="toc-text">4. Cosine Similarity</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Diversification"><span class="toc-number">5.</span> <span class="toc-text">5. Diversification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Max-Sum-Similarity（最大相似度）"><span class="toc-number">5.1.</span> <span class="toc-text">5.1  Max Sum Similarity（最大相似度）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Maximal-Marginal-Relevance（最大边际相关性）"><span class="toc-number">5.2.</span> <span class="toc-text">5.2 Maximal Marginal Relevance（最大边际相关性）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#参考"><span class="toc-number"></span> <span class="toc-text">参考</span></a></div></section><!-- ov = overview --><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/banner/labia.jpeg" alt="avatar"></div><p class="sidebar-ov-author__p">The Harder You Work, The Luckier You Will Be.</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social__item" href="https://github.com/Lvshaomei/Lvshaomei.github.io" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><i class="sidebar-ov-social__item--icon fa fa-github"></i></a><a class="sidebar-ov-social__item" href="https://segmentfault.com/u/lvlv_5e182fd7c6adc" target="_blank" rel="noopener" data-popover="思否" data-popover-pos="up"><span class="sidebar-ov-social__item--logo">sf</span></a><a class="sidebar-ov-social__item" href="https://www.zhihu.com/people/fa-liang-jian-shao/activities" target="_blank" rel="noopener" data-popover="知乎" data-popover-pos="up"><span class="sidebar-ov-social__item--logo">�</span></a><a class="sidebar-ov-social__item" href="https://juejin.im/user/5da86e3d5188257948602ace" target="_blank" rel="noopener" data-popover="social.juejin" data-popover-pos="up"><span class="sidebar-ov-social__item--logo">�</span></a></div><div class="sidebar-ov-feed"><span class="sidebar-ov-feed-rss"><a class="sidebar-ov-feed-rss__a" href="../../../../atom.xml" target="_blank" rel="noopener"><i class="sidebar-ov-feed-rss__a--icon fa fa-rss"></i><span>RSS </span><span>订阅</span></a></span></div><div class="sidebar-ov-state"></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-cn" target="_blank" rel="noopener" data-popover="知识共享" data-popover-pos="up"><img src="../../../../images/cc-by-nc-sa.svg" alt="知识共享"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span>你已阅读了 </span><span class="sidebar-reading-info-num">0</span></div><div class="sidebar-reading-line"></div></div></div></aside><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>&copy; 2023</span><span class="fa fa-heart footer-icon"></span><span>LSM.</span></div><div><span>由 <a href="http://hexo.io/" title="hexo" target="_blank" rel="noopener">hexo</a> 强力驱动</span><span> v4.0.0.</span><span class="separator">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="stun" target="_blank" rel="noopener">stun</a></span><span> v1.5.4.</span></div><div class="busuanzi"><span class="busuanzi-site_uv"><i class="fa fa-user"></i><span>访问人数 </span><span id="busuanzi_value_site_uv"></span></span><span class="separator">|</span><span class="busuanzi-site_pv"><i class="fa fa-eye"></i><span>浏览总量 </span><span id="busuanzi_value_site_pv"></span></span><script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@latest/bsz.pure.mini.js" async></script></div></div></footer><div class="loading-bar" id="loading-bar"><div class="progress"></div></div><div class="back2top" id="back2top"><i class="back2top-icon fa fa-rocket"></i></div></div><div class="search-mask"></div><div class="search-popup"><span class="search-close"></span><div class="search-input"><input placeholder="搜索文章（支持多关键词，请用空格分隔）"></div><div class="search-results"></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/ribbon.js@latest/dist/ribbon.min.js" size="120" alpha="0.6" zIndex="-1"></script><script src="https://cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@latest/canvas-nest.min.js" color="243,81,151" opacity="0.5" count="100" zIndex="-2"></script><script>window.addEventListener('DOMContentLoaded', function () {
  $('.header-nav-search').on('click', function (e) {
    e.stopPropagation();

    $('body').css('overflow', 'hidden');
    $('.search-popup')
      .addClass('show')
      .velocity('stop')
      .velocity('transition.expandIn', {
        duration: 300,
        complete: function () {
          $('.search-popup input').focus();
        }
      });
    $('.search-mask')
      .velocity('stop')
      .velocity('transition.fadeIn', {
        duration: 300
      });

    initSearch();
  });

  $('.search-mask, .search-close').on('click', function () {
    closeSearch();
  });

  $(document).on('keydown', function (e) {
    // Escape <=> 27
    if (e.keyCode === Stun.utils.codeToKeyCode('Escape')) {
      closeSearch();
    }
  });

  var isXML = true;
  var search_path = 'search.xml';

  if (!search_path) {
    search_path = 'search.xml';
  } else if (/json$/i.test(search_path)) {
    isXML = false;
  }

  var path = '/' + search_path;

  function initSearch() {
    $.ajax({
      url: path,
      dataType: isXML ? 'xml' : 'json',
      async: true,
      success: function (res) {
        var datas = isXML ? $('entry', res).map(function () {
          // 将 XML 转为 JSON
          return {
            title: $('title', this).text(),
            content: $('content', this).text(),
            url: $('url', this).text()
          };
        }).get() : res;

        var $input = $('.search-input input');
        var $result = $('.search-results');

        // 搜索对象（标题、内容）的权重，影响显示顺序
        var WEIGHT = { title: 100, content: 1 };

        var searchPost = function () {
          var searchText = $input.val().toLowerCase().trim();
          // 根据空白字符分隔关键字
          var keywords = searchText.split(/[\s]+/);
          // 搜索结果
          var matchPosts = [];

          // 有多个关键字时，将原文字整个保存下来
          if (keywords.length > 1) {
            keywords.push(searchText);
          }

          // 防止未输入字符时搜索
          if (searchText.length > 0) {
            datas.forEach(function (data) {
              var isMatch  = false;

              // 没有标题的文章使用预设的 i18n 变量代替
              var title = (data.title && data.title.trim()) || '( 文章无标题 )';
              var titleLower = title && title.toLowerCase();
              // 删除 HTML 标签 和 所有空白字符
              var content = data.content && data.content.replace(/<[^>]+>/g, '');
              var contentLower = content && content.toLowerCase();
              // 删除重复的 /
              var postURL = data.url && decodeURI(data.url).replace(/\/{2,}/g, '/');

              // 标题中匹配到的关键词
              var titleHitSlice = [];
              // 内容中匹配到的关键词
              var contentHitSlice = [];

              keywords.forEach(function (keyword) {
                /**
                  * 获取匹配的关键词的索引
                  * @param {String} keyword 要匹配的关键字
                  * @param {String} text 原文字
                  * @param {Boolean} caseSensitive 是否区分大小写
                  * @param {Number} weight 匹配对象的权重。权重大的优先显示
                  * @return {Array}
                  */
                function getIndexByword (word, text, caseSensitive, weight) {
                  if (!word || !text) return [];

                  var startIndex = 0; // 每次匹配的开始索引
                  var index = -1;     // 匹配到的索引值
                  var result = [];    // 匹配结果

                  if (!caseSensitive) {
                    word = word.toLowerCase();
                    text = text.toLowerCase();
                  }

                  while((index = text.indexOf(word, startIndex)) !== -1) {
                    var hasMatch = false;

                    // 索引位置相同的关键词，保留长度较长的
                    titleHitSlice.forEach(function (hit) {
                      if (hit.index === index && hit.word.length < word.length) {
                        hit.word = word;
                        hasMatch = true;
                      }
                    });
                    startIndex = index + word.length;
                    !hasMatch && result.push({ index: index, word: word, weight: weight });
                  }

                  return result;
                }

                titleHitSlice = titleHitSlice.concat(getIndexByword(keyword, titleLower, false, WEIGHT.title));
                contentHitSlice = contentHitSlice.concat(getIndexByword(keyword, contentLower, false, WEIGHT.content));
              });

              var hitTitle = titleHitSlice.length;
              var hitContent = contentHitSlice.length;

              if (hitTitle > 0 || hitContent > 0) {
                isMatch = true;
              }

              if (isMatch) {
                ;[titleHitSlice, contentHitSlice].forEach(function (hit) {
                  // 按照匹配文字的索引的递增顺序排序
                  hit.sort(function (left, right) {
                    return left.index - right.index;
                  });
                });

                /**
                  * 给文本中匹配到的关键词添加标记，从而进行高亮显示
                  * @param {String} text 原文本
                  * @param {Array} hitSlice 匹配项的索引信息
                  * @param {Number} start 开始索引
                  * @param {Number} end 结束索引
                  * @return {String}
                  */
                function highlightKeyword (text, hitSlice, start, end) {
                  if (!text || !hitSlice || !hitSlice.length) return;

                  var result = '';
                  var startIndex = start;
                  var endIndex = end;

                  hitSlice.forEach(function (hit) {
                    if (hit.index < startIndex) return;

                    var hitWordEnd = hit.index + hit.word.length;

                    result += text.slice(startIndex, hit.index);
                    result += '<b>' + text.slice(hit.index, hitWordEnd) + '</b>';
                    startIndex = hitWordEnd;
                  });
                  result += text.slice(startIndex, endIndex);

                  return result;
                }

                var postData = {};
                // 文章总的搜索权重
                var postWeight = titleHitSlice.length * WEIGHT.title + contentHitSlice.length * WEIGHT.content;
                // 标记匹配关键词后的标题
                var postTitle = highlightKeyword(title, titleHitSlice, 0, title.length) || title;
                // 标记匹配关键词后的内容
                var postContent;
                // 显示内容的长度
                var SHOW_WORD_LENGTH = 200;
                // 命中关键词前的字符显示长度
                var SHOW_WORD_FRONT_LENGTH = 20;
                var SHOW_WORD_END_LENGTH = SHOW_WORD_LENGTH - SHOW_WORD_FRONT_LENGTH;

                // 截取匹配的第一个字符，前后共 200 个字符来显示
                if (contentHitSlice.length > 0) {
                  var firstIndex = contentHitSlice[0].index;
                  var start = firstIndex > SHOW_WORD_FRONT_LENGTH ? firstIndex - SHOW_WORD_FRONT_LENGTH : 0;
                  var end = firstIndex + SHOW_WORD_END_LENGTH;

                  postContent = highlightKeyword(content, contentHitSlice, start, end);
                } else { // 未匹配到内容，直接截取前 200 个字符来显示
                  postContent = content.slice(0, SHOW_WORD_LENGTH);
                }

                postData.title = postTitle;
                postData.content = postContent;
                postData.url = postURL;
                postData.weight = postWeight;
                matchPosts.push(postData);
              }
            });
          }

          var resultInnerHtml = '';

          if (matchPosts.length) {
            // 按权重递增的顺序排序，使权重大的优先显示
            matchPosts.sort(function (left, right) {
              return right.weight - left.weight;
            });

            resultInnerHtml += '<ul>';
            matchPosts.forEach(function (post) {
              resultInnerHtml += '<li><a class="search-results-title" href="' + post.url + '">';
              resultInnerHtml += post.title;
              resultInnerHtml += '</a><div class="search-results-content">';
              resultInnerHtml += post.content;
              resultInnerHtml += '</div></li>';
            });
            resultInnerHtml += '</ul>';
          } else {
            resultInnerHtml += '<div class="search-results-none"><i class="fa fa-meh-o"></i></div>';
          }

          $result.html(resultInnerHtml);
        };

        $input.on('input', searchPost);
        $input.on('keypress', function (e) {
          if (e.keyCode === Stun.utils.codeToKeyCode('Enter')) {
            searchPost();
          }
        });
      }
    });
  }

  function closeSearch () {
    $('body').css('overflow', 'auto');
    $('.search-popup')
      .removeClass('show')
      .velocity('stop')
      .velocity('transition.expandOut', {
        duration: 300
      });
    $('.search-mask')
      .velocity('stop')
      .velocity('transition.fadeOut', {
        duration: 300
      });
  }
}, false);</script><script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@latest/bsz.pure.mini.js" async></script><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-md5@latest/src/md5.min.js"></script><script>function loadGitalk () {
  if (!document.getElementById('gitalk-container')) {
    return;
  }

  var gitalk = new Gitalk({
    id: md5(window.location.pathname.slice(1)),
    clientID: '23d7b4a6c1b1589b9d9b',
    clientSecret: 'fcbbb62bf5fe3fac81979e69fd71cd637e0c523f',
    repo: 'Lvshaomei.github.io',
    owner: 'Lvshaomei',
    admin: ['Lvshaomei'],
    distractionFreeMode: 'true',
    language: 'zh-CN'
  });

  gitalk.render('gitalk-container');
}

if (false) {
  loadGitalk();
} else {
  window.addEventListener('DOMContentLoaded', loadGitalk, false);
}</script><script src="../../../../js/utils.js?v=1.5.4"></script><script src="../../../../js/stun-boot.js?v=1.5.4"></script><script src="../../../../js/scroll.js?v=1.5.4"></script><script src="../../../../js/header.js?v=1.5.4"></script><script src="../../../../js/sidebar.js?v=1.5.4"></script></body></html>